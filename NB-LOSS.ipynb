{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karen/anaconda3/lib/python3.6/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n",
      "/Users/karen/anaconda3/lib/python3.6/site-packages/scipy/sparse/sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    }
   ],
   "source": [
    "#import required packages\n",
    "#basic\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "#misc\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "#viz\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec \n",
    "import seaborn as sns\n",
    "import pyLDAvis.gensim\n",
    "#nlp\n",
    "import string\n",
    "import re     #for regex\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "import gensim\n",
    "from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "\n",
    "#Modeling\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_is_fitted\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import sparse\n",
    "\n",
    "#settings\n",
    "start_time=time.time()\n",
    "color = sns.color_palette()\n",
    "sns.set_style(\"dark\")\n",
    "\n",
    "#constants\n",
    "eng_stopwords = set(stopwords.words(\"english\"))\n",
    "#settings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "lem = WordNetLemmatizer()\n",
    "tokenizer=ToktokTokenizer()\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time till import: 1.6451141834259033 s\n"
     ]
    }
   ],
   "source": [
    "start_time=time.time()\n",
    "#importing the dataset\n",
    "train=pd.read_csv(\"/Users/karen/Desktop/datamining/cw1/jigsaw-toxic-comment-classification-challenge/train.csv\")\n",
    "test=pd.read_csv(\"/Users/karen/Desktop/datamining/cw1/jigsaw-toxic-comment-classification-challenge/test.csv\")\n",
    "end_import=time.time()\n",
    "print(\"Time till import:\",end_import-start_time,\"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to seperate sentenses into words\n",
    "def preprocess(comment):\n",
    "    \"\"\"\n",
    "    Function to build tokenized texts from input comment\n",
    "    \"\"\"\n",
    "    return gensim.utils.simple_preprocess(comment, deacc=True, min_len=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time till pre-process: 57.17487096786499 s\n"
     ]
    }
   ],
   "source": [
    "#tokenize the comments\n",
    "train_text=train.comment_text.apply(lambda x: preprocess(x))\n",
    "test_text=test.comment_text.apply(lambda x: preprocess(x))\n",
    "all_text=train_text.append(test_text)\n",
    "end_preprocess=time.time()\n",
    "print(\"Time till pre-process:\",end_preprocess-start_time,\"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         [explanation, why, the, edits, made, under, us...\n",
      "1         [aww, matches, this, background, colour, seemi...\n",
      "2         [hey, man, really, not, trying, edit, war, jus...\n",
      "3         [more, can, make, any, real, suggestions, impr...\n",
      "4         [you, sir, are, hero, any, chance, you, rememb...\n",
      "5         [congratulations, from, well, use, the, tools,...\n",
      "6             [cocksucker, before, you, piss, around, work]\n",
      "7         [your, vandalism, the, matt, shirvington, arti...\n",
      "8         [sorry, the, word, nonsense, was, offensive, y...\n",
      "9         [alignment, this, subject, and, which, are, co...\n",
      "10        [fair, use, rationale, for, image, wonju, jpg,...\n",
      "11        [bbq, man, and, lets, discuss, maybe, over, th...\n",
      "12        [hey, what, talk, what, exclusive, group, some...\n",
      "13        [before, you, start, throwing, accusations, an...\n",
      "14        [and, the, girl, above, started, her, argument...\n",
      "15        [juelz, santanas, age, juelz, santana, was, ye...\n",
      "16        [bye, don, look, come, think, comming, back, t...\n",
      "17        [redirect, talk, voydan, pop, georgiev, cherno...\n",
      "18        [the, mitsurugi, point, made, sense, why, not,...\n",
      "19        [don, mean, bother, you, see, that, you, writi...\n",
      "20        [regarding, your, recent, edits, once, again, ...\n",
      "21          [good, know, about, yeah, studying, now, deepu]\n",
      "22        [snowflakes, are, not, always, symmetrical, un...\n",
      "23        [the, signpost, september, read, this, signpos...\n",
      "24        [considering, paragraph, edit, don, understand...\n",
      "25        [radial, symmetry, several, now, extinct, line...\n",
      "26        [there, need, apologize, wikipedia, article, m...\n",
      "27        [yes, because, the, mother, the, child, the, c...\n",
      "28        [but, will, take, bit, work, but, can, quite, ...\n",
      "29        [barnstar, for, you, the, real, life, barnstar...\n",
      "                                ...                        \n",
      "153134    [same, coffee, shop, memory, shaky, and, maybe...\n",
      "153135    [many, things, wrong, with, that, viewpoint, f...\n",
      "153136    [unless, have, article, for, some, other, batt...\n",
      "153137    [hannah, and, maddie, are, soooooo, awesome, a...\n",
      "153138    [problem, tagged, and, cleaned, out, anyone, c...\n",
      "153139    [just, looked, the, history, this, article, th...\n",
      "153140    [your, edit, maungaturoto, please, don, try, a...\n",
      "153141    [you, wish, contest, the, prod, please, remove...\n",
      "153142    [balancing, the, two, approaches, psychiatry, ...\n",
      "153143                                        [suck, balls]\n",
      "153144    [your, name, mentioned, just, thought, letting...\n",
      "153145    [just, discovered, yet, another, list, list, c...\n",
      "153146    [wikiproject, video, games, assessment, don, h...\n",
      "153147    [consensus, for, ruining, wikipedia, think, th...\n",
      "153148    [dap, what, point, with, dap, nazism, isn, soc...\n",
      "153149    [shut, down, the, mexican, border, withought, ...\n",
      "153150    [jerome, see, you, never, got, around, this, n...\n",
      "153151    [lucky, bastard, http, org, wiki, press_releas...\n",
      "153152      [wtf, longer, redlink, now, what, the, problem]\n",
      "153153    [illness, shit, just, for, the, record, you, c...\n",
      "153154    [shame, you, all, you, want, speak, about, gay...\n",
      "153155    [mel, gibson, nazi, bitch, who, makes, shitty,...\n",
      "153156    [unicorn, lair, discovery, supposedly, unicorn...\n",
      "153157    [disagree, soviet, railways, need, their, own,...\n",
      "153158    [this, idiot, can, even, use, proper, grammar,...\n",
      "153159    [totally, agree, this, stuff, nothing, but, to...\n",
      "153160    [throw, from, out, field, home, plate, does, g...\n",
      "153161    [okinotorishima, categories, see, your, change...\n",
      "153162    [one, the, founding, nations, the, germany, ha...\n",
      "153163    [stop, already, your, bullshit, not, welcome, ...\n",
      "Name: comment_text, Length: 312735, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#checks\n",
    "#print(\"Total number of comments:\",len(all_text))\n",
    "#print(\"Before preprocessing:\",train.comment_text.iloc[30])\n",
    "print(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Phrases help us group together bigrams :  new + york --> new_york\n",
    "bigram = gensim.models.Phrases(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['how',\n",
       " 'could',\n",
       " 'post',\n",
       " 'before',\n",
       " 'the',\n",
       " 'block_expires',\n",
       " 'the',\n",
       " 'funny_thing',\n",
       " 'you',\n",
       " 'think',\n",
       " 'being_uncivil']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check bigram collation functionality \n",
    "bigram[all_text.iloc[30]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(word_list):\n",
    "    \"\"\"\n",
    "    Function to clean the pre-processed word lists \n",
    "    \n",
    "    Following transformations will be done\n",
    "    1) Stop words removal from the nltk stopword list\n",
    "    2) Bigram collation (Finding common bigrams and grouping them together using gensim.models.phrases)\n",
    "    3) Lemmatization (Converting word to its root form : babies --> baby ; children --> child)\n",
    "    \"\"\"\n",
    "    #remove stop words\n",
    "    clean_words = [w for w in word_list if not w in eng_stopwords]\n",
    "    #collect bigrams\n",
    "    clean_words = bigram[clean_words]\n",
    "    #Lemmatize\n",
    "    clean_words=[lem.lemmatize(word, \"v\") for word in clean_words]\n",
    "    return(clean_words)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/karen/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Before clean: ['aww', 'matches', 'this', 'background', 'colour', 'seemingly', 'stuck', 'with', 'thanks', 'talk', 'january', 'utc']\n",
      "After clean: ['aww', 'match', 'background', 'colour', 'seemingly', 'stick', 'thank', 'talk', 'january_utc']\n",
      "0         [explanation, why, the, edits, made, under, us...\n",
      "1         [aww, matches, this, background, colour, seemi...\n",
      "2         [hey, man, really, not, trying, edit, war, jus...\n",
      "3         [more, can, make, any, real, suggestions, impr...\n",
      "4         [you, sir, are, hero, any, chance, you, rememb...\n",
      "5         [congratulations, from, well, use, the, tools,...\n",
      "6             [cocksucker, before, you, piss, around, work]\n",
      "7         [your, vandalism, the, matt, shirvington, arti...\n",
      "8         [sorry, the, word, nonsense, was, offensive, y...\n",
      "9         [alignment, this, subject, and, which, are, co...\n",
      "10        [fair, use, rationale, for, image, wonju, jpg,...\n",
      "11        [bbq, man, and, lets, discuss, maybe, over, th...\n",
      "12        [hey, what, talk, what, exclusive, group, some...\n",
      "13        [before, you, start, throwing, accusations, an...\n",
      "14        [and, the, girl, above, started, her, argument...\n",
      "15        [juelz, santanas, age, juelz, santana, was, ye...\n",
      "16        [bye, don, look, come, think, comming, back, t...\n",
      "17        [redirect, talk, voydan, pop, georgiev, cherno...\n",
      "18        [the, mitsurugi, point, made, sense, why, not,...\n",
      "19        [don, mean, bother, you, see, that, you, writi...\n",
      "20        [regarding, your, recent, edits, once, again, ...\n",
      "21          [good, know, about, yeah, studying, now, deepu]\n",
      "22        [snowflakes, are, not, always, symmetrical, un...\n",
      "23        [the, signpost, september, read, this, signpos...\n",
      "24        [considering, paragraph, edit, don, understand...\n",
      "25        [radial, symmetry, several, now, extinct, line...\n",
      "26        [there, need, apologize, wikipedia, article, m...\n",
      "27        [yes, because, the, mother, the, child, the, c...\n",
      "28        [but, will, take, bit, work, but, can, quite, ...\n",
      "29        [barnstar, for, you, the, real, life, barnstar...\n",
      "                                ...                        \n",
      "153134    [same, coffee, shop, memory, shaky, and, maybe...\n",
      "153135    [many, things, wrong, with, that, viewpoint, f...\n",
      "153136    [unless, have, article, for, some, other, batt...\n",
      "153137    [hannah, and, maddie, are, soooooo, awesome, a...\n",
      "153138    [problem, tagged, and, cleaned, out, anyone, c...\n",
      "153139    [just, looked, the, history, this, article, th...\n",
      "153140    [your, edit, maungaturoto, please, don, try, a...\n",
      "153141    [you, wish, contest, the, prod, please, remove...\n",
      "153142    [balancing, the, two, approaches, psychiatry, ...\n",
      "153143                                        [suck, balls]\n",
      "153144    [your, name, mentioned, just, thought, letting...\n",
      "153145    [just, discovered, yet, another, list, list, c...\n",
      "153146    [wikiproject, video, games, assessment, don, h...\n",
      "153147    [consensus, for, ruining, wikipedia, think, th...\n",
      "153148    [dap, what, point, with, dap, nazism, isn, soc...\n",
      "153149    [shut, down, the, mexican, border, withought, ...\n",
      "153150    [jerome, see, you, never, got, around, this, n...\n",
      "153151    [lucky, bastard, http, org, wiki, press_releas...\n",
      "153152      [wtf, longer, redlink, now, what, the, problem]\n",
      "153153    [illness, shit, just, for, the, record, you, c...\n",
      "153154    [shame, you, all, you, want, speak, about, gay...\n",
      "153155    [mel, gibson, nazi, bitch, who, makes, shitty,...\n",
      "153156    [unicorn, lair, discovery, supposedly, unicorn...\n",
      "153157    [disagree, soviet, railways, need, their, own,...\n",
      "153158    [this, idiot, can, even, use, proper, grammar,...\n",
      "153159    [totally, agree, this, stuff, nothing, but, to...\n",
      "153160    [throw, from, out, field, home, plate, does, g...\n",
      "153161    [okinotorishima, categories, see, your, change...\n",
      "153162    [one, the, founding, nations, the, germany, ha...\n",
      "153163    [stop, already, your, bullshit, not, welcome, ...\n",
      "Name: comment_text, Length: 312735, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#check clean function\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "print(\"Before clean:\",all_text.iloc[1])\n",
    "print(\"After clean:\",clean(all_text.iloc[1]))\n",
    "print(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "#scale it to all text\n",
    "all_text=all_text.apply(lambda x:clean(x))\n",
    "end_clean=time.time()\n",
    "#print(\"Time till cleaning corpus:\",end_clean-start_time,\"s\")\n",
    "print(type(all_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(322843 unique tokens: ['closure', 'dolls', 'edit', 'explanation', 'fac']...)\n"
     ]
    }
   ],
   "source": [
    "#create the dictionary\n",
    "dictionary = Dictionary(all_text)\n",
    "print(dictionary)\n",
    "#print(\"There are\",len(dictionary),\"number of words in the final dictionary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1)]\n",
      "Wordlist from the sentence: ['aww', 'match', 'background', 'colour', 'seemingly', 'stick', 'thank', 'talk', 'january_utc']\n",
      "Wordlist from the dictionary lookup: aww background colour january_utc match seemingly stick\n"
     ]
    }
   ],
   "source": [
    "#convert into lookup tuples within the dictionary using doc2bow\n",
    "print(dictionary.doc2bow(all_text.iloc[1]))\n",
    "print(\"Wordlist from the sentence:\",all_text.iloc[1])\n",
    "#to check\n",
    "print(\"Wordlist from the dictionary lookup:\", \n",
    "      dictionary[21],dictionary[22],dictionary[23],dictionary[24],dictionary[25],dictionary[26],dictionary[27])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time till corpus creation: 176.90189003944397 s\n"
     ]
    }
   ],
   "source": [
    "#scale it to all text\n",
    "corpus = [dictionary.doc2bow(text) for text in all_text]\n",
    "end_corpus=time.time()\n",
    "print(\"Time till corpus creation:\",end_clean-start_time,\"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time till LDA model creation: 360.1368000507355 s\n"
     ]
    }
   ],
   "source": [
    "#create the LDA model\n",
    "ldamodel = LdaModel(corpus=corpus, num_topics=15, id2word=dictionary)\n",
    "end_lda=time.time()\n",
    "print(\"Time till LDA model creation:\",end_lda-start_time,\"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#end_viz=time.time()\n",
    "#print(\"Time till viz:\",end_viz-start_time,\"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the topic probability matrix \n",
    "topic_probability_mat = ldamodel[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split it to test and train\n",
    "train_matrix=topic_probability_mat[:train.shape[0]]\n",
    "test_matrix=topic_probability_mat[train.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del(topic_probability_mat)\n",
    "del(corpus)\n",
    "del(all_text)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time till Sparse mat creation 463.2603120803833 s\n"
     ]
    }
   ],
   "source": [
    "#convert to sparse format (Csr matrix)\n",
    "train_sparse=gensim.matutils.corpus2csc(train_matrix)\n",
    "test_sparse=gensim.matutils.corpus2csc(test_matrix)\n",
    "end_time=time.time()\n",
    "print(\"total time till Sparse mat creation\",end_time-start_time,\"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom NB model\n",
    "class NbSvmClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0, dual=False, n_jobs=1):\n",
    "        self.C = C\n",
    "        self.dual = dual\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    def predict(self, x):\n",
    "        # Verify that model has been fit\n",
    "        check_is_fitted(self, ['_r', '_clf'])\n",
    "        return self._clf.predict(x.multiply(self._r))\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        # Verify that model has been fit\n",
    "        check_is_fitted(self, ['_r', '_clf'])\n",
    "        return self._clf.predict_proba(x.multiply(self._r))\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        # Check that X and y have correct shape\n",
    "        y = y.values\n",
    "        x, y = check_X_y(x, y, accept_sparse=True)\n",
    "\n",
    "        def pr(x, y_i, y):\n",
    "            p = x[y==y_i].sum(0)\n",
    "            return (p+1) / ((y==y_i).sum()+1)\n",
    "\n",
    "        self._r = sparse.csr_matrix(np.log(pr(x,1,y) / pr(x,0,y)))\n",
    "        x_nb = x.multiply(self._r)\n",
    "        self._clf = LogisticRegression(C=self.C, dual=self.dual, n_jobs=self.n_jobs).fit(x_nb, y)\n",
    "        return self\n",
    "    \n",
    "\n",
    "model = NbSvmClassifier(C=2, dual=True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the target columns\n",
    "target_x=train_sparse.transpose()\n",
    "TARGET_COLS=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "target_y=train[TARGET_COLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del(train_sparse)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class:= toxic\n",
      "Trainloss=log loss: 0.24875156155448505\n",
      "Validloss=log loss: 0.24859783520421386\n",
      "Class:= severe_toxic\n",
      "Trainloss=log loss: 0.04213695052989292\n",
      "Validloss=log loss: 0.04326405536329918\n",
      "Class:= obscene\n",
      "Trainloss=log loss: 0.1578500852145476\n",
      "Validloss=log loss: 0.15775597008347342\n",
      "Class:= threat\n",
      "Trainloss=log loss: 0.016821525405965217\n",
      "Validloss=log loss: 0.016563809523861245\n",
      "Class:= insult\n",
      "Trainloss=log loss: 0.15174119952283305\n",
      "Validloss=log loss: 0.1533719794504035\n",
      "Class:= identity_hate\n",
      "Trainloss=log loss: 0.04004948560782748\n",
      "Validloss=log loss: 0.04315805746136871\n",
      "mean column-wise log loss:Train dataset 0.1095584679725919\n",
      "mean column-wise log loss:Validation dataset 0.1104519511811033\n",
      "total time till NB base model creation 479.46180415153503\n"
     ]
    }
   ],
   "source": [
    "model = NbSvmClassifier(C=4, dual=True, n_jobs=-1)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(target_x, target_y, test_size=0.33, random_state=2018)\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "preds_train = np.zeros((X_train.shape[0], y_train.shape[1]))\n",
    "preds_valid = np.zeros((X_valid.shape[0], y_train.shape[1]))\n",
    "for i, j in enumerate(TARGET_COLS):\n",
    "    print('Class:= '+j)\n",
    "    model.fit(X_train,y_train[j])\n",
    "    preds_valid[:,i] = model.predict_proba(X_valid)[:,1]\n",
    "    preds_train[:,i] = model.predict_proba(X_train)[:,1]\n",
    "    train_loss_class=log_loss(y_train[j],preds_train[:,i])\n",
    "    valid_loss_class=log_loss(y_valid[j],preds_valid[:,i])\n",
    "    print('Trainloss=log loss:', train_loss_class)\n",
    "    print('Validloss=log loss:', valid_loss_class)\n",
    "    train_loss.append(train_loss_class)\n",
    "    valid_loss.append(valid_loss_class)\n",
    "print('mean column-wise log loss:Train dataset', np.mean(train_loss))\n",
    "print('mean column-wise log loss:Validation dataset', np.mean(valid_loss))\n",
    "\n",
    "\n",
    "end_time=time.time()\n",
    "print(\"total time till NB base model creation\",end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
